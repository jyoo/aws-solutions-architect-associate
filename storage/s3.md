# Features
- Available: 99.5% to 99.99% (availability is about ensuring uptime)
- Durable: 11 9's (durability is about the probability of keeping your data without losing data through corruption, degradation or etc.)
- Lifecycle policy: transition actions (from one storage class to the other) / expiration actions (remove an object after a certain period of time)
- Ownership: By default, each S3 object is owned by the account that creates / uploads the object
    - Ownership can be changed by updating its ACL to `bucket-owner-full-control`
    - This is not the best practice. Grant programmatic cross-account permission
- Notification: You can send notification to SNS, SQS and Lambda
    - Object creation / removal / restoration
    - Reduced redundancy storage object lost
    - Replication
- Versioning: Unversioned / Versioning-enabled / Versioning-suspended
- Server access logging: Detailed records for the requests made to a bucket / object
    - Every logging may not be captured
    - A bucket for server access logging must be in the same region as your targeted bucket
    - Specific write permissions are required, and they can be configured via Access Control List
    - If encryption is enabled on your bucket, access logs can be delivered only if it is SSE-S3 (No support for SSE-KMS)

# Limitation
- Each object can be from 0 bytes to 5TB
- You can have up to 100 buckets (soft limit)
- 5,500 requests for GET and HEAD. 3,500 for other types of operations (per prefix)
- If a S3 bucket uses KMS, different quota should be considered. (5,000, 10,000 and 30,000 requests per second)

# Pricing / Classes
## Standard
- Standard

## Infrequent Access
- Standard
- One zone
- Same low latency and high throughput. Nine 9s durability and one 9 availability

## For archiving / compliance
- Glacier 
- Glacier deep archive

## etc.
- Intelligent - Tiering 

## Pricing components
- You will be charged for all bandwidth into and out of S3 except:
    - Data transferred in from the Internet
    - Data transferred out to Amazon Cloudfront
    - Data transferred out to an Amazon EC2 instance in the same region as the S3 bucket (including a different account)

# Transfer acceleration
- It allows a user to transfer files over long distances between your client and S3 bucket fast in a secure manner using Amazon Cloudfront's Edge locations
- To use acceleration, your bucket name must be DNS compliant without any period in its name
- Acceleration does not support:
    - GET (list buckets)
    - PUT (create bucket)
    - DELETE bucket
    - Cross region copies using PUT object

# Archive Retrieval Options
- Standard
    - Default option
    - 3-5 hours for Glacier Flexible / Intelligent-Tiering Archive
    - Up to 12 hours for Glaicer Deep Archive / Intelligent-Tiering Deep Archive
- Expedited
    - Object in S3 Glacier Flexible Retrieval or Intelligent-Archive Tiering Access
    - No more than 250+ MB, 1-5 minutes processing time
    - Provisioned capacity is helpful to ensure fast retrieval
- Bulk
    - 5 to 48 hours to process a request

# Lock / Compliance
## Features
- Retention periods
- Legal holds: It remains in effect until removed (it can be set by an owner with `s3:PutObjectLegalHold`)
- Glacier vault lock: it can't be overwritten once configured

## S3 Object Lock modes
- Governance mode: It allows only some users (with permission) to overwrite or delete an object version or to update lock settings
- Compliance mode: Protected objects can't be overwritten by any user (including the root user) - they can't also update the lock settings

# Resource-based policy
- For identity-based policy, take a look at security/IAM.md
- Example of resource-based policies: access control list and bucket policy
- Access control list:
    - Access control list: Control of access to bucket and specific objects within a bucket 
    - It does not allow implicitly denying or using conditional statements
    - Types: bucket owner / signed requests (authenticated) / unsigned requests (unauthenticated)
- Bucket policy:
    - You must specify a 'Principal' element that defines the principal who is associated with the action
    - Example: Controlling S3 buckets and objects / Cross-account access feature without creating or assuming IAM roles

# Encryption
- Server-side encryption
    - Amazon S3-managed keys (SSE-S3): S3 managed keys using AES 256-bit encryption
    - AWS KMS-managed keys (SSE-KMS): providing the audit trail feature
    - Customer-provided keys (SSE-C)
- Client-side encryption
    - AWS KMS-managed keys
    - Customer-supplied client-side master keys
- `x-amz-server-side-encryption` must be provided to enforce server-side encryption
- The following should be provided for SSE-C:
    - `x-amz-server-side-encryption-customer-algorithm`
    - `x-amz-server-side-encryption-customer-key`
    - `x-amz-server-side-encryption-customer-key-MDS`

# Replication / Backup
- Cross-region replication is available to deal with one or multiple AZ failure or entire region failure
    - Objects created before replicion are not replicated automatically
    - Delete markers are not replicated by default
    - Deleting a replicated object in a replicated bucket does not result in affect its original object in a bucket

# Related Products
- Amazon Athena: Running simple SQL queries against a subset of data included in a S3 object
- S3 Select: Retrieve a subset of data using SQL queries when data is stored in csv files of zip file format in S3
    - S3 Glacier Select

# Additional Notes
- `s3:PutObject` does not allow deletion of objects. IAM roles: `s3:PutObject`, `s3:GetObject` and `s3:DeleteObject`
- If a static website will be hosted on a S3 bucket, the bucket's name must be the same as a domain or subdomain
- CORS is NOT necessary unless the web application powered by a S3 bucket requires interaction with other external resources
- Cross-account access does not replicate data from one region to another
- Read after write consistency for PUTS of new objects
- Eventual consistency for overwrite PUTS and DELETES
- Multipart uplodates to parallelize uploads of a file 100+ MB
- Byte-Range fetches to parallelize downloads
